{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce6fd3e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final Exam\n",
    "\n",
    "- Open book, open note, you can use your laptop but no internet access.\n",
    "- December 13,Â 1:30-3:20 p.m. in Old Main 326\n",
    "- Questions\n",
    "    - Multiple choice\n",
    "    - Spot the mistakes in code\n",
    "    - Short answer questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0cac08",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f277a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Supervised learning vs. Unsupervised learning\n",
    "\n",
    "In **supervised learning**, training data comprises a set of features ($X$) and their corresponding targets ($y$). We wish to find a **model function $f$** that relates $X$ to $y$. Then use that model function **to predict the targets** of new examples. \n",
    "\n",
    "\n",
    "![](img/sup-learning.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada3b13a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In **unsupervised learning** training data consists of observations ($X$) **without any corresponding targets**. Unsupervised learning could be used to **group similar things together** in $X$ or to provide **concise summary** of the data. We'll learn more about this topic in later videos.\n",
    "\n",
    "![](img/unsup-learning.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6a45f4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train/validation/test split\n",
    "\n",
    "- Some of you may have heard of \"**validation**\" data.\n",
    "- Sometimes it's a good idea to have a separate data for **hyperparameter tuning**.\n",
    "\n",
    "![](img/train-valid-test-split.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a7f99e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We will try to use \"validation\" to refer to data where we have access to the target values.\n",
    "  - But, unlike the training data,\n",
    "    - we only use this for hyperparameter tuning and model assessment; \n",
    "    - we don't pass these into `fit`.\n",
    "- We will try to use \"test\" to refer to data where we have access to the target values \n",
    "  - But, unlike training and validation data,\n",
    "    - we neither use it in training nor hyperparameter optimization. \n",
    "  - We only use it **once** to evaluate the performance of the best performing model on the validation set.   \n",
    "  - We lock it in a \"vault\" until we're ready to evaluate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491ed32a",
   "metadata": {},
   "source": [
    "### Underfitting, overfitting, the fundamental trade-off, the golden rule\n",
    "\n",
    "#### Underfitting\n",
    "- If your **model is too simple**, like `DummyClassifier` or `DecisionTreeClassifier` with `max_depth=1`, it's not going to pick up on some random quirks in the data but it won't even capture useful patterns in the training data.\n",
    "- The model won't be very good in general. Both train and validation errors would be high. This is **underfitting**.\n",
    "- The gap between train and validation error is going to be lower.\n",
    "\n",
    "#### Overfitting\n",
    "- If your **model is very complex**, like a `DecisionTreeClassifier(max_depth=None)`, then you will learn unreliable patterns in order to get every single training example correct.\n",
    "- The training error is going to be very low but there will be a big gap between the training error and the validation error. This is **overfitting**.\n",
    "\n",
    "#### The \"fundamental tradeoff\" of supervised learning:\n",
    "**As you increase model complexity, $E_\\textrm{train}\\ $ tends to go down but $E_\\textrm{valid}-E_\\textrm{train}\\ $ tends to go up.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afec542",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to pick a model that would generalize better?\n",
    "\n",
    "- We want to avoid both underfitting and overfitting. \n",
    "- We want to be consistent with the training data but we don't want to rely too much on it. \n",
    "\n",
    "<!-- <center>\n",
    "<img src='img/malp_0201.png' width=\"800\" height=\"800\" />\n",
    "</center>    \n",
    " -->\n",
    "![](img/malp_0201.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecceb78",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The golden rule\n",
    "\n",
    "- Even though we care the most about test error **THE TEST DATA CANNOT INFLUENCE THE TRAINING PHASE IN ANY WAY**. \n",
    "- We have to be very careful not to violate it while developing our ML pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a882c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Here is the workflow we'll generally follow. \n",
    "\n",
    "- **Splitting**: Before doing anything, split the data `X` and `y` into `X_train`, `X_test`, `y_train`, `y_test` or `train_df` and `test_df` using `train_test_split`. \n",
    "- **Select the best model using cross-validation**: Use `cross_validate` with `return_train_score = True` so that we can get access to training scores in each fold. (If we want to plot train vs validation error plots, for instance.) \n",
    "- **Scoring on test data**: Finally score on the test data with the chosen hyperparameters to examine the generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba173b4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](img/gridsearch_workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514d2620",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hyperparameter Optimization\n",
    "\n",
    "- Exhaustive grid search: [`sklearn.model_selection.GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "    - Required number of models to evaluate grows exponentially with the dimensionally of the configuration space. \n",
    "    - Example: Suppose you have\n",
    "        - 5 hyperparameters \n",
    "        - 10 different values for each hyperparameter\n",
    "        - You'll be evaluating $10^5=100,000$ models! That is you'll be calling `cross_validate` 100,000 times!\n",
    "    - Exhaustive search may become infeasible fairly quickly. \n",
    "\n",
    "- Randomized search: [`sklearn.model_selection.RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "    - Faster compared to `GridSearchCV`.\n",
    "    - Adding parameters that do not influence the performance does not affect efficiency.\n",
    "    - Works better when some parameters are more important than others. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab473ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification metrics\n",
    "\n",
    "<img src='./img/evaluation-metrics.png' width=\"1000\" height=\"1000\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb05684",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Receiver Operating Characteristic (ROC) curve \n",
    "\n",
    "- Another commonly used tool to analyze the behavior of classifiers at different thresholds.  \n",
    "- Similar to PR curve, it considers all possible thresholds for a given classifier given by `predict_proba` but instead of precision and recall it plots false positive rate (FPR) and true positive rate (TPR or recall).\n",
    "$$ FPR  = \\frac{FP}{FP + TN}, TPR = \\frac{TP}{TP + FP}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aa71f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression metrics\n",
    "\n",
    "A number of popular scoring functions for regression. We are going to look at some common metrics: \n",
    "\n",
    "- mean squared error (MSE)\n",
    "- $R^2$\n",
    "    - This is the score that `sklearn` uses by default when you call [score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html):\n",
    "- root mean squared error (RMSE)\n",
    "- MAPE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78906536",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### K-Means\n",
    "\n",
    "- Represent each cluster by its cluster center and assign a cluster membership to each data point. \n",
    "\n",
    "\n",
    "- algorithm\n",
    "\n",
    "    - **Input**: Data points X and the number of clusters K\n",
    "\n",
    "    - **Initialization**: K initial centers for the clusters\n",
    "\n",
    "    - **Iterative process**:\n",
    "\n",
    "    - repeat \n",
    "        - Assign each example to the closest center.\n",
    "        - Estimate new centers as _average_ of observations in a cluster.\n",
    "\n",
    "    - until **centers stop changing** or **maximum iterations have reached**.\n",
    "    \n",
    "\n",
    "- Hyperparameter tuning for K\n",
    "    - The Elbow method\n",
    "        - This method looks at the sum of **intra-cluster distances**, which is also referred to as **inertia**. \n",
    "        - The intra-cluster distance in our toy example above is given as   \n",
    "\n",
    "        $$\\sum_{P_i \\in C_1}  distance(P_i, C_1)^2 + \\sum_{P_i \\in C_2}  distance(P_i, C_2)^2 + \\sum_{P_i \\in C_3} distance(P_i, C_3)^2$$\n",
    "\n",
    "        Where \n",
    "        - $C_1, C_2, C_3$ are centroids \n",
    "        - $P_i$s are points within that cluster\n",
    "        - $distance$ is the usual Euclidean distance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdce545",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hierarchical clustering\n",
    "\n",
    "- Main idea\n",
    "    1. Start with each point in its own cluster. \n",
    "    2. Greedily merge most similar *clusters*. \n",
    "    3. Repeat Step 2 until you obtain only one cluster ($n-1$ times).\n",
    "    \n",
    "- Dendrogram\n",
    "    - Dendrogram is a tree-like plot. \n",
    "    - On the x-axis we have data points. \n",
    "    - On the y-axis we have distances between clusters. \n",
    "    - We start with data points as leaves of the tree.  \n",
    "    - New parent node is created for every two clusters that are joined. \n",
    "    - The length of each branch shows how far the merged clusters go. \n",
    "        - In the dendrogram above going from three clusters to two clusters means merging far apart points because the branches between three cluster to two clusters are long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd826b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
